{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"f07df259-d439-4d89-a24f-b78b0731eab4","_cell_guid":"5190cf31-5714-4733-adc7-64ffa78d452d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:13:25.075721Z","iopub.execute_input":"2022-09-06T13:13:25.076186Z","iopub.status.idle":"2022-09-06T13:13:41.392567Z","shell.execute_reply.started":"2022-09-06T13:13:25.076098Z","shell.execute_reply":"2022-09-06T13:13:41.391615Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nfrom torchvision.utils import make_grid\nfrom torchvision.utils import save_image\n\nfrom torch.utils.data import random_split\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\n\nimport time\n\nimport glob","metadata":{"_uuid":"98e1f5cd-7985-426a-a47b-ac2f1d729f28","_cell_guid":"2e5edcce-8cf5-4354-bc9e-fee9fe194cab","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:13:41.397660Z","iopub.execute_input":"2022-09-06T13:13:41.399885Z","iopub.status.idle":"2022-09-06T13:13:43.996746Z","shell.execute_reply.started":"2022-09-06T13:13:41.399846Z","shell.execute_reply":"2022-09-06T13:13:43.995707Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Print out used versions\nprint(\"Used versions:\")\nprint(f\"Torch:\\t\\t{torch.__version__}\")\nprint(f\"Torchvision:\\t{torchvision.__version__}\")\nprint()","metadata":{"_uuid":"f7d70613-aa32-4fda-84a0-81fe32ab1f94","_cell_guid":"a0c5e405-6f9a-4027-b34a-95ac454ea56c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:13:43.999327Z","iopub.execute_input":"2022-09-06T13:13:43.999904Z","iopub.status.idle":"2022-09-06T13:13:44.006525Z","shell.execute_reply.started":"2022-09-06T13:13:43.999867Z","shell.execute_reply":"2022-09-06T13:13:44.005479Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def get_device():\n    \"\"\"\n    Check if GPU is available, and if so, picks the GPU, else picks the CPU\n    \"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n\n# Set the device to cuda is available\ndevice = get_device()\nprint(f\"Used device is {device}.\")","metadata":{"_uuid":"eccdd1b5-115a-4946-b34a-b9929b42c92d","_cell_guid":"ec3f93e8-ec56-4b7d-a39c-0564a7115a83","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:13:44.009541Z","iopub.execute_input":"2022-09-06T13:13:44.010483Z","iopub.status.idle":"2022-09-06T13:13:44.089155Z","shell.execute_reply.started":"2022-09-06T13:13:44.010448Z","shell.execute_reply":"2022-09-06T13:13:44.087921Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_num_correct(preds, labels):\n    \"\"\"\n    Returns the number of the correctly predicted images\n        Parameters:\n            preds (tensor): the predicted labels\n            labels (tensor): the true labels (targets)\n        Returns:\n            num_correct (int): the number of correctly predicted images\n    \"\"\"\n    num_correct = preds.argmax(dim=1).eq(labels).sum().item()\n    return num_correct","metadata":{"_uuid":"b875dd08-c851-4aa8-831d-d57e3c98077f","_cell_guid":"94bb52af-b019-47ad-b46d-5aeb4dcd7cc2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:13:44.092951Z","iopub.execute_input":"2022-09-06T13:13:44.095167Z","iopub.status.idle":"2022-09-06T13:13:44.101124Z","shell.execute_reply.started":"2022-09-06T13:13:44.095134Z","shell.execute_reply":"2022-09-06T13:13:44.100088Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def accuracy(preds, labels):\n    \"\"\"\n    Returns the accuracy of the predictions\n        Parameters:\n            preds (tensor): the predicted labels\n            labels (tensor): the true labels (targets)\n        Returns:\n            acc (float): the accuracy of the predictions (correctly predicted labels / all predictions)\n    \"\"\"\n    acc = get_num_correct(preds, labels) / len(labels)\n    return acc","metadata":{"_uuid":"06f36134-26e4-4b9b-9ed3-b1ff94578bc6","_cell_guid":"225f5cce-10d1-454e-a058-355e529df866","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:13:44.102650Z","iopub.execute_input":"2022-09-06T13:13:44.103078Z","iopub.status.idle":"2022-09-06T13:13:44.115219Z","shell.execute_reply.started":"2022-09-06T13:13:44.103038Z","shell.execute_reply":"2022-09-06T13:13:44.114394Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def predict(network, image):\n    \"\"\"\n    Returns the prediction of an image\n        Parameters:\n            network (model): the model to use\n            image (tensor): the image to predict the label for\n        Returns:\n            pred (int): the predicted label of the image\n    \"\"\"\n    network = network.to(device)\n    image = image.to(device)\n    output = network(image.unsqueeze(0))\n    pred = output.argmax(dim=1).item()\n    return pred","metadata":{"_uuid":"bf5fb3c4-bca4-4961-bf31-73b7e3fadae0","_cell_guid":"31b145dc-641b-4e65-81f8-e49c26868e2f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:13:44.116903Z","iopub.execute_input":"2022-09-06T13:13:44.117646Z","iopub.status.idle":"2022-09-06T13:13:44.127508Z","shell.execute_reply.started":"2022-09-06T13:13:44.117610Z","shell.execute_reply":"2022-09-06T13:13:44.126518Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_ds_normal_path = glob.glob(\"../input/chest-xray-pneumonia/chest_xray/train/NORMAL/*\")\ntrain_ds_pneumonia_path = glob.glob(\"../input/chest-xray-pneumonia/chest_xray/train/PNEUMONIA/*\")\n\ntest_ds_normal_path = glob.glob(\"../input/chest-xray-pneumonia/chest_xray/test/NORMAL/*\")\ntest_ds_pneumonia_path = glob.glob(\"../input/chest-xray-pneumonia/chest_xray/test/PNEUMONIA/*\")\n\nval_ds_normal_path = glob.glob(\"../input/chest-xray-pneumonia/chest_xray/val/NORMAL/*\")\nval_ds_pneumonia_path = glob.glob(\"../input/chest-xray-pneumonia/chest_xray/val/PNEUMONIA/*\")","metadata":{"_uuid":"e7116865-f599-4c57-9091-2adf46581224","_cell_guid":"669abb9b-8039-483c-8fef-91757b411016","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:13:44.129466Z","iopub.execute_input":"2022-09-06T13:13:44.129908Z","iopub.status.idle":"2022-09-06T13:13:44.159992Z","shell.execute_reply.started":"2022-09-06T13:13:44.129872Z","shell.execute_reply":"2022-09-06T13:13:44.159100Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(f\"'Normal' images in training set: \\t{len(train_ds_normal_path):6,}\")\nprint(f\"'Pneumonia' images in training set: \\t{len(train_ds_pneumonia_path):6,}\")\nprint()\n\nprint(f\"'Normal' images in validation set: \\t{len(val_ds_normal_path):6,}\")\nprint(f\"'Pneumonia' images in validation set: \\t{len(val_ds_pneumonia_path):6,}\")\nprint()\n\nprint(f\"'Normal' images in test set: \\t\\t{len(test_ds_normal_path):6,}\")\nprint(f\"'Pneumonia' images in test set: \\t{len(test_ds_pneumonia_path):6,}\")","metadata":{"_uuid":"323c33c0-09c7-416e-ac39-ff800b6cc243","_cell_guid":"70e056ff-61c2-4702-9efd-bed09a973a1a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:13:44.161602Z","iopub.execute_input":"2022-09-06T13:13:44.162221Z","iopub.status.idle":"2022-09-06T13:13:44.169281Z","shell.execute_reply.started":"2022-09-06T13:13:44.162185Z","shell.execute_reply":"2022-09-06T13:13:44.168240Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Since the validation set is basically empty, and after a lot of trying and readings on forums, the test set is likely to be labelled incorrectly, \n# the validation and test sets will be separated from the training set, which will be the only folder in use\n\n# In the training set there are about 3 times as many pneumonia images as normal images, which could cause nonoptimal learning\n# With data augmentation the number of normal images can be increased","metadata":{"_uuid":"3da018ba-b641-4e95-8503-52cd80d0c330","_cell_guid":"5771c813-2675-4dee-a8a3-8ce4596c7f26","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:13:44.174914Z","iopub.execute_input":"2022-09-06T13:13:44.175554Z","iopub.status.idle":"2022-09-06T13:13:44.182307Z","shell.execute_reply.started":"2022-09-06T13:13:44.175524Z","shell.execute_reply":"2022-09-06T13:13:44.181238Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"d3e2a2b1-037e-4268-8336-3466f3711e34","_cell_guid":"0686cb9e-fd1f-4420-acc7-88d536fe4a68","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 10% of normal and pneumonia images is separated from training set -> test set\n\nnormal_test_size = 300\nnormal_val_size = 200\nnormal_train_size = len(train_ds_normal_path) - normal_test_size - normal_val_size\n\ntrain_ds_normal_path, test_ds_normal_path, val_ds_normal_path = random_split(train_ds_normal_path, [normal_train_size, normal_test_size, normal_val_size])\n\npneumonia_test_size = 300\npneumonia_val_size = 200\npneumonia_train_size = len(train_ds_pneumonia_path) - pneumonia_test_size - pneumonia_val_size\n\ntrain_ds_pneumonia_path, test_ds_pneumonia_path, val_ds_pneumonia_path = random_split(train_ds_pneumonia_path, [pneumonia_train_size, pneumonia_test_size, pneumonia_val_size])","metadata":{"_uuid":"9488bb4e-4678-49b8-8736-64900b86290f","_cell_guid":"dcee54f2-9d80-455a-bc0a-0a4791fd678b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:13:44.183986Z","iopub.execute_input":"2022-09-06T13:13:44.184925Z","iopub.status.idle":"2022-09-06T13:13:44.193511Z","shell.execute_reply.started":"2022-09-06T13:13:44.184884Z","shell.execute_reply":"2022-09-06T13:13:44.192561Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"09aee6f8-4004-4382-9007-bb555004e24e","_cell_guid":"3d21c9ef-a2f3-4f5c-9c9c-cb92b56e7f0a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print()\nprint(f\"Images in datasets:\")\nprint(f\"TRAINING set size: \\t{normal_train_size:5,} + {pneumonia_train_size:5,}\")\nprint(f\"VALIDATION set size: \\t{normal_val_size:5,} + {pneumonia_val_size:5,}\")\nprint(f\"TEST set size: \\t\\t{normal_test_size:5,} + {pneumonia_test_size:5,}\")","metadata":{"_uuid":"4783499b-5743-4326-ae51-a1ab4cb9a695","_cell_guid":"9bd2261f-fe7d-4074-803b-535b4d0622df","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:13:44.194634Z","iopub.execute_input":"2022-09-06T13:13:44.197440Z","iopub.status.idle":"2022-09-06T13:13:44.204182Z","shell.execute_reply.started":"2022-09-06T13:13:44.197395Z","shell.execute_reply":"2022-09-06T13:13:44.203002Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class CustomImages(torch.utils.data.Dataset):\n    \"\"\"\n    Opens images from their paths and returns transformed images\n          Parameters:\n              paths (list): the list of image paths\n              image_transformations (transforms): the transformations we want to apply on the images\n          Returns:\n              image (image): the current modified image\n    \"\"\"\n\n    def __init__(self, path, image_transformations=None):\n        self.path = path\n        self.image_transformations = image_transformations\n\n    def __getitem__(self, idx):\n        # dataset[idx]\n        path = self.path[idx]\n        image = Image.open(path).convert(\"RGB\")\n        if self.image_transformations:\n            image = self.image_transformations(image)\n            \n        return image\n\n    def __len__(self):\n        # len(dataset)\n        return len(self.path)","metadata":{"_uuid":"113f8828-0934-45a1-8020-98d979656ac0","_cell_guid":"b600f233-8e57-409b-a31a-0a70fe459fb7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:13:44.206099Z","iopub.execute_input":"2022-09-06T13:13:44.206936Z","iopub.status.idle":"2022-09-06T13:13:44.214684Z","shell.execute_reply.started":"2022-09-06T13:13:44.206847Z","shell.execute_reply":"2022-09-06T13:13:44.213756Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Transformations for the augmented normal images\naugmentation_transformations = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(5),\n    transforms.ColorJitter(brightness=.05),\n    transforms.RandomPerspective(distortion_scale=0.1, p=0.1),\n    transforms.ToTensor()\n])\n\n\ndataset = CustomImages(train_ds_normal_path, augmentation_transformations)","metadata":{"_uuid":"111c757a-5242-4184-9011-25fd4b5319ea","_cell_guid":"1149b547-ec17-41ca-9d45-6d7d4800052f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:13:44.216320Z","iopub.execute_input":"2022-09-06T13:13:44.217124Z","iopub.status.idle":"2022-09-06T13:13:44.227637Z","shell.execute_reply.started":"2022-09-06T13:13:44.217089Z","shell.execute_reply":"2022-09-06T13:13:44.226667Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"target_dir = r\"../working/augmented\"\nif not os.path.exists(target_dir):\n    os.mkdir(\"../working/augmented\")\n\n\ncounter = 0\nfor i in range(2):\n    for image in dataset:\n        \n        filepath = os.path.join(target_dir, f\"normal_augmented_{counter:04n}.jpg\")\n        \n        save_image(image, filepath)\n        counter += 1","metadata":{"_uuid":"ef0567ad-32bb-4e1c-84e9-1e9f82a729a5","_cell_guid":"a1267b61-b365-49d6-8662-10e1e392a499","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:13:44.229317Z","iopub.execute_input":"2022-09-06T13:13:44.230041Z","iopub.status.idle":"2022-09-06T13:20:19.165627Z","shell.execute_reply.started":"2022-09-06T13:13:44.229932Z","shell.execute_reply":"2022-09-06T13:20:19.164547Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_ds_normal_augmented_path = glob.glob(\"../working/augmented/*\")\n\n\nnormal_train_size += len(train_ds_normal_augmented_path)","metadata":{"_uuid":"63466d13-09d4-4e1d-83a1-000209ac6d63","_cell_guid":"60d2547c-e6b8-459b-bbfa-314d6e1d433e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:20:19.166915Z","iopub.execute_input":"2022-09-06T13:20:19.167294Z","iopub.status.idle":"2022-09-06T13:20:19.180163Z","shell.execute_reply.started":"2022-09-06T13:20:19.167260Z","shell.execute_reply":"2022-09-06T13:20:19.179319Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print()\nprint(f\"Images in datasets after augmenting:\")\nprint(f\"TRAINING set size: \\t{normal_train_size:5,} + {pneumonia_train_size:5,}\")\nprint(f\"VALIDATION set size: \\t{normal_val_size:5,} + {pneumonia_val_size:5,}\")\nprint(f\"TEST set size: \\t\\t{normal_test_size:5,} + {pneumonia_test_size:5,}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-06T13:20:19.181455Z","iopub.execute_input":"2022-09-06T13:20:19.182353Z","iopub.status.idle":"2022-09-06T13:20:19.188765Z","shell.execute_reply.started":"2022-09-06T13:20:19.182300Z","shell.execute_reply":"2022-09-06T13:20:19.187812Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_ds_normal_labels = torch.zeros(len(train_ds_normal_path) + len(train_ds_normal_augmented_path))\ntest_ds_normal_labels = torch.zeros(len(test_ds_normal_path))\nval_ds_normal_labels = torch.zeros(len(val_ds_normal_path))\n\ntrain_ds_pneumonia_labels = torch.ones(len(train_ds_pneumonia_path))\ntest_ds_pneumonia_labels = torch.ones(len(test_ds_pneumonia_path))\nval_ds_pneumonia_labels = torch.ones(len(val_ds_pneumonia_path))","metadata":{"_uuid":"eef2a23d-3741-4038-911d-b8fb28f47b87","_cell_guid":"e9c3b986-42a7-4dd0-9db0-343530475aa6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:20:19.190127Z","iopub.execute_input":"2022-09-06T13:20:19.191069Z","iopub.status.idle":"2022-09-06T13:20:19.199415Z","shell.execute_reply.started":"2022-09-06T13:20:19.191024Z","shell.execute_reply":"2022-09-06T13:20:19.198373Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_labels = torch.cat((train_ds_normal_labels, train_ds_pneumonia_labels), dim=0)\n\ntest_labels = torch.cat((test_ds_normal_labels, test_ds_pneumonia_labels), dim=0)\n\nval_labels = torch.cat((val_ds_normal_labels, val_ds_pneumonia_labels), dim=0)","metadata":{"_uuid":"d4d20f02-e80d-479f-bf13-3418067eef4f","_cell_guid":"303dcfeb-9740-434c-b1a4-cdbc1a5a3be2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:20:19.201626Z","iopub.execute_input":"2022-09-06T13:20:19.202403Z","iopub.status.idle":"2022-09-06T13:20:19.212457Z","shell.execute_reply.started":"2022-09-06T13:20:19.202367Z","shell.execute_reply":"2022-09-06T13:20:19.211471Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_path = train_ds_normal_path + train_ds_normal_augmented_path + train_ds_pneumonia_path\n\ntest_path = test_ds_normal_path + test_ds_pneumonia_path\n\nval_path = val_ds_normal_path + val_ds_pneumonia_path","metadata":{"_uuid":"f8e13d54-3780-4a19-8c40-e08b557b514e","_cell_guid":"0ac6203b-bec2-41bb-9d41-a5618b6e9373","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:20:19.214013Z","iopub.execute_input":"2022-09-06T13:20:19.214451Z","iopub.status.idle":"2022-09-06T13:20:19.223052Z","shell.execute_reply.started":"2022-09-06T13:20:19.214417Z","shell.execute_reply":"2022-09-06T13:20:19.221969Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"e2a7f1c3-1eee-4451-ba85-8d236d974305","_cell_guid":"63a9256f-3c38-46f7-a86c-6869a0492f56","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MergeDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Returns merged dataset with image path - label pairs\n          Parameters:\n              paths (list): the list of image paths\n              labels (list): the list of labels\n          Returns:\n              image, label (tuple): the current modified image-label tuple\n    \"\"\"\n\n    def __init__(self, paths, labels):\n        self.paths = paths\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        # dataset[idx]\n        path = self.paths[idx]\n        image = Image.open(path).convert(\"RGB\")\n        label = self.labels[idx]\n\n        return image, label\n\n    def __len__(self):\n        # len(dataset)\n        return len(self.paths)","metadata":{"_uuid":"85ebb7ee-9bb5-488c-b8ce-3b2ca6db77f5","_cell_guid":"4f181b34-d6aa-4d5a-973b-2fc1974c4533","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:20:19.224685Z","iopub.execute_input":"2022-09-06T13:20:19.225167Z","iopub.status.idle":"2022-09-06T13:20:19.234590Z","shell.execute_reply.started":"2022-09-06T13:20:19.225133Z","shell.execute_reply":"2022-09-06T13:20:19.233565Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_dataset = MergeDataset(train_path, train_labels)\nval_dataset = MergeDataset(val_path, val_labels)\ntest_dataset = MergeDataset(test_path, test_labels)","metadata":{"_uuid":"3daa1b2c-2fe7-4c30-9c7d-bae21c1746d9","_cell_guid":"6d1a050b-2cf3-407b-a976-b63cc2bf9fff","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:20:19.236880Z","iopub.execute_input":"2022-09-06T13:20:19.237712Z","iopub.status.idle":"2022-09-06T13:20:19.244783Z","shell.execute_reply.started":"2022-09-06T13:20:19.237687Z","shell.execute_reply":"2022-09-06T13:20:19.243715Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"7e13df68-be50-45af-83ff-11376f634e18","_cell_guid":"3e6c996f-58a1-4f43-b8ab-70f189170d39","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PneumoniaDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Opens images from their paths and returns transformed dataset\n          Parameters:\n              paths (list): the list of image paths\n              labels (list): the list of labels\n              image_transformations (transforms): the transformations we want to apply on the images\n              label_transformations (transforms): the transformations we want to apply on the labels\n          Returns:\n              image, label (tuple): the current modified image-label tuple\n    \"\"\"\n\n    def __init__(self, dataset, image_transformations=None, label_transformations=None):\n        self.dataset = dataset\n        self.image_transformations = image_transformations\n        self.label_transformations = label_transformations\n\n    def __getitem__(self, idx):\n        # dataset[idx]\n        image, label = self.dataset[idx]\n        if self.image_transformations:\n            image = self.image_transformations(image)\n        if self.label_transformations:\n            label = self.label_transformations(label)\n\n        return image, label\n\n    def __len__(self):\n        # len(dataset)\n        return len(self.dataset)","metadata":{"_uuid":"1a09ec71-32b8-4081-825c-e0b3b229952e","_cell_guid":"7f06015d-3a48-4587-8fd1-126c540f8918","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:20:19.247174Z","iopub.execute_input":"2022-09-06T13:20:19.247671Z","iopub.status.idle":"2022-09-06T13:20:19.256119Z","shell.execute_reply.started":"2022-09-06T13:20:19.247637Z","shell.execute_reply":"2022-09-06T13:20:19.255248Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Create a dataloader for sample images to show\nshow_dataset = PneumoniaDataset(train_dataset, image_transformations=transforms.Compose([transforms.Resize((300, 300)), transforms.ToTensor()]))\nshow_dataloader = DataLoader(show_dataset, batch_size=16, shuffle=True)","metadata":{"_uuid":"807c0827-be3d-4cd5-9862-69af3626e561","_cell_guid":"a4bdcb8b-e4f7-433c-a57d-8a09a3d8e91d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:20:19.258114Z","iopub.execute_input":"2022-09-06T13:20:19.258890Z","iopub.status.idle":"2022-09-06T13:20:19.270089Z","shell.execute_reply.started":"2022-09-06T13:20:19.258856Z","shell.execute_reply":"2022-09-06T13:20:19.269062Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"classes = {\n            0 : \"Normal\",\n            1 : \"Pneumonia\"\n          }\n\nfor images, labels in show_dataloader:\n    plt.figure(figsize=(30,30))\n    plt.axis(\"off\")\n    plt.imshow(make_grid(images, nrow=4, padding=0).permute(1,2,0))\n    for index, label in enumerate(labels):\n        if index < 4:\n            plt.text(index*300+6, 18, classes[label.item()], bbox={'facecolor': 'white', 'pad': 10}, fontsize=20)\n        elif index < 8:\n            plt.text((index-4)*300+6, 318, classes[label.item()], bbox={'facecolor': 'white', 'pad': 10}, fontsize=20)\n        elif index < 12:\n            plt.text((index-8)*300+6, 618, classes[label.item()], bbox={'facecolor': 'white', 'pad': 10}, fontsize=20)\n        else:\n            plt.text((index-12)*300+6, 918, classes[label.item()], bbox={'facecolor': 'white', 'pad': 10}, fontsize=20)\n    break","metadata":{"_uuid":"3bc15f0e-e4ac-44ca-8b41-8eee2cdafe9d","_cell_guid":"9b2672f0-715a-4f4a-9203-5d121da06864","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:20:19.271244Z","iopub.execute_input":"2022-09-06T13:20:19.274554Z","iopub.status.idle":"2022-09-06T13:20:21.759533Z","shell.execute_reply.started":"2022-09-06T13:20:19.274520Z","shell.execute_reply":"2022-09-06T13:20:21.758262Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Set image size for transformations\nimage_size = (256, 256)\n\n# Set training|validation|test transformations\ntraining_transformations = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.RandomRotation(degrees=10),\n    transforms.RandomPerspective(distortion_scale=0.1, p=0.1),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(size=image_size, padding=4),\n    transforms.ToTensor()\n])\n\nvalidation_transformations = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor()\n])\n\ntest_transformations = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor()\n])","metadata":{"_uuid":"d6445b2e-4197-49a3-9831-1f27afcaba89","_cell_guid":"ce9eed87-2ecd-44d7-a535-34b3afd94283","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:20:21.760866Z","iopub.execute_input":"2022-09-06T13:20:21.761318Z","iopub.status.idle":"2022-09-06T13:20:21.769921Z","shell.execute_reply.started":"2022-09-06T13:20:21.761268Z","shell.execute_reply":"2022-09-06T13:20:21.769067Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Apply transformations\ntrain_ds = PneumoniaDataset(train_dataset, training_transformations)\ntrain_dataset_size = len(train_ds)\n\nval_ds = PneumoniaDataset(val_dataset, validation_transformations)\nval_dataset_size = len(val_ds)\n\ntest_ds = PneumoniaDataset(test_dataset, test_transformations)\ntest_dataset_size = len(test_ds)","metadata":{"_uuid":"ffa3572d-af20-4f71-8837-d423bd47238e","_cell_guid":"16c3ff6a-8452-4883-90c5-ab404251139b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:20:21.771494Z","iopub.execute_input":"2022-09-06T13:20:21.772112Z","iopub.status.idle":"2022-09-06T13:20:21.780478Z","shell.execute_reply.started":"2022-09-06T13:20:21.772072Z","shell.execute_reply":"2022-09-06T13:20:21.779386Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nMODEL DEFINITION 1\n\"\"\"\n\nclass model1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        \n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.batchnorm_conv_1 = nn.BatchNorm2d(32)\n        self.maxpool2d_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout1 = nn.Dropout(0.20)\n        \n        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.batchnorm_conv_2 = nn.BatchNorm2d(32)\n        self.maxpool2d_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.batchnorm_conv_3 = nn.BatchNorm2d(64)\n        self.maxpool2d_3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout2 = nn.Dropout(0.20)\n        \n        self.conv5 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.maxpool2d_4 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.fc1 = nn.Linear(in_features=64 * int(image_size[0]/16) * int(image_size[1]/16), out_features=1024)\n        self.dropout3 = nn.Dropout(0.30)\n        \n        self.fc2 = nn.Linear(in_features=1024, out_features=256)\n        \n        self.out = nn.Linear(in_features=256, out_features=2)\n        \n    def forward(self, t):\n        t = F.relu(self.conv1(t))\n        t = self.dropout1(self.maxpool2d_1(F.relu(self.batchnorm_conv_1(self.conv2(t)))))\n        t = self.maxpool2d_2(F.relu(self.batchnorm_conv_2(self.conv3(t))))\n        t = self.dropout2(self.maxpool2d_3(F.relu(self.batchnorm_conv_3(self.conv4(t)))))\n        t = self.maxpool2d_4(F.relu(self.conv5(t)))\n        \n        t = torch.flatten(t, start_dim=1)\n        #t = t.reshape(-1, 64 * int(image_size[0]/16) * int(image_size[1]/16))\n        t = self.dropout3(F.relu(self.fc1(t)))\n        t = F.relu(self.fc2(t))\n        t = F.relu(self.out(t))\n        \n        return t","metadata":{"_uuid":"f3082f50-96a7-41bf-9138-552def5f9b4d","_cell_guid":"e2d8da44-81d8-4005-958b-cccef7197448","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:20:21.788617Z","iopub.execute_input":"2022-09-06T13:20:21.789154Z","iopub.status.idle":"2022-09-06T13:20:21.928674Z","shell.execute_reply.started":"2022-09-06T13:20:21.789122Z","shell.execute_reply":"2022-09-06T13:20:21.927422Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nMODEL DEFINITION 2\n\"\"\"\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=(5, 5), stride=(1, 1),\n                               padding=(2, 2))\n        self.conv2 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=(5, 5), stride=(1, 1),\n                               padding=(2, 2))\n\n    def forward(self, t):\n        res_t = t\n        t = self.conv1(t)\n        t = F.relu(t)\n        t = self.conv2(t)\n        t += res_t\n        t = F.relu(t)\n\n        return t\n    \n    \nmodel2 = nn.Sequential(\n    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),\n    nn.ReLU(),\n    nn.BatchNorm2d(16),\n    nn.Conv2d(in_channels=16, out_channels=64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),\n    nn.ReLU(),\n    nn.BatchNorm2d(64),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    ResidualBlock(channels=64),\n    ResidualBlock(channels=64),\n    ResidualBlock(channels=64),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),\n    nn.ReLU(),\n    nn.BatchNorm2d(128),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    ResidualBlock(channels=128),\n    ResidualBlock(channels=128),\n    ResidualBlock(channels=128),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),\n    nn.ReLU(),\n    nn.BatchNorm2d(128),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    ResidualBlock(channels=128),\n    ResidualBlock(channels=128),\n    ResidualBlock(channels=128),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),\n    nn.ReLU(),\n    nn.BatchNorm2d(256),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Flatten(start_dim=1),\n    nn.Linear(in_features=256 * int(image_size[0]/128) * int(image_size[1]/128), out_features=2048),\n    nn.ReLU(),\n    nn.BatchNorm1d(2048),\n    nn.Linear(in_features=2048, out_features=1024),\n    nn.ReLU(),\n    nn.BatchNorm1d(1024),\n    nn.Linear(in_features=1024, out_features=256),\n    nn.ReLU(),\n    nn.BatchNorm1d(256),\n    nn.Linear(in_features=256, out_features=64),\n    nn.ReLU(),\n    nn.BatchNorm1d(64),\n    nn.Linear(in_features=64, out_features=2)\n)","metadata":{"_uuid":"15834c22-2c70-4431-8127-f7bb9fc51260","_cell_guid":"94e86673-a828-45ba-aee5-32014c6e82ae","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:20:21.932087Z","iopub.execute_input":"2022-09-06T13:20:21.933039Z","iopub.status.idle":"2022-09-06T13:20:22.045581Z","shell.execute_reply.started":"2022-09-06T13:20:21.933000Z","shell.execute_reply":"2022-09-06T13:20:22.044535Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nHYPERPARAMETERS\n\"\"\"\n\nnetwork = model1()\n\nbatch_size = 32\nloss_fn = F.cross_entropy\nlearning_rate = 0.0005\nnum_epoch = 25\n\nnetwork.to(device)\n\nprint(network)\nprint()\npytorch_total_params = sum(p.numel() for p in network.parameters())\nprint(f\"Total parameters: \\t\\t{pytorch_total_params:,}\")\npytorch_total_learnable_params = sum(p.numel() for p in network.parameters() if p.requires_grad)\nprint(f\"Total trainable parameters: \\t{pytorch_total_learnable_params:,}\")\nprint()\nprint(f\"Training dataset size: \\t\\t{train_dataset_size:,}\")\nprint(f\"Validation dataset size: \\t{val_dataset_size:,}\")\nprint(f\"Test dataset size: \\t\\t{test_dataset_size:,}\")\nprint()\nprint(f\"Batch size    \\t {batch_size}\")\nprint(f\"Learning rate \\t {learning_rate}\")\nprint(f\"Loss function \\t {loss_fn}\")\nprint(f\"No. epochs    \\t {num_epoch}\")\n\n\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size * 2)\ntest_loader = DataLoader(test_ds, batch_size * 2)\n\n\noptimizer = optim.Adam(network.parameters(), lr=learning_rate)\n\nsched = optim.lr_scheduler.OneCycleLR(optimizer, learning_rate, epochs=num_epoch, steps_per_epoch=len(train_loader))","metadata":{"_uuid":"c20f144b-01a4-4988-bd00-7bdfc7933409","_cell_guid":"755f77d5-b715-401e-9741-4f46238a77b0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:45:39.656697Z","iopub.execute_input":"2022-09-06T13:45:39.657060Z","iopub.status.idle":"2022-09-06T13:45:39.870724Z","shell.execute_reply.started":"2022-09-06T13:45:39.657028Z","shell.execute_reply":"2022-09-06T13:45:39.869624Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nBEFORE TRAINING METRICS\n\"\"\"\n\nhistory_train_loss = []\nhistory_train_acc = []\n\nhistory_val_loss = []\nhistory_val_acc = []\n\ntotal_loss = 0\ntotal_correct = 0\n\ntotal_val_loss = 0\ntotal_val_correct = 0\n\n\nnetwork.eval()\nfor batch in train_loader:\n    images, labels = batch\n\n    images = images.to(device)\n    labels = labels.to(device)\n    labels = labels.to(torch.int64)\n\n    preds = network(images)\n    loss = loss_fn(preds, labels)\n\n    total_loss += loss.item() * len(batch[0])\n    total_correct += get_num_correct(preds, labels)\n\nfor batch in val_loader:\n    images, labels = batch\n\n    images = images.to(device)\n    labels = labels.to(device)\n    labels = labels.to(torch.int64)\n\n    preds = network(images)\n    loss = loss_fn(preds, labels)\n\n    total_val_loss += loss.item() * len(batch[0])\n    total_val_correct += get_num_correct(preds, labels)\n\nprint()\nprint(f\"BEFORE TRAINING: train accuracy {100 * total_correct / train_dataset_size:3.2f}%, \"\n      f\"loss = {total_loss / train_dataset_size:4.4f}\")\nprint(f\"BEFORE TRAINING: val accuracy   {100 * total_val_correct / val_dataset_size:3.2f}%, \"\n      f\"loss = {total_val_loss / val_dataset_size:4.4f}\")\nprint()\nprint()\n\nhistory_train_acc.append(100 * total_correct / train_dataset_size)\nhistory_val_acc.append(100 * total_val_correct / val_dataset_size)\n\nhistory_train_loss.append(total_loss / train_dataset_size)\nhistory_val_loss.append(total_val_loss / val_dataset_size)","metadata":{"_uuid":"75b99c88-eb33-4e9c-8846-9337b8b7fe63","_cell_guid":"fabf3e8b-c447-44e4-a15c-60734fd2b3aa","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:45:40.522868Z","iopub.execute_input":"2022-09-06T13:45:40.523219Z","iopub.status.idle":"2022-09-06T13:48:48.371093Z","shell.execute_reply.started":"2022-09-06T13:45:40.523189Z","shell.execute_reply":"2022-09-06T13:48:48.370020Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nTRAINING\n\"\"\"\nsince = time.time()\n\ntorch.cuda.empty_cache()\n\nfor epoch in range(num_epoch):\n\n    # Training phase\n\n    total_loss = 0\n    total_correct = 0\n\n    network.train()\n\n    for batch in train_loader:\n        images, labels = batch\n\n        images = images.to(device)\n        labels = labels.to(device)\n        labels = labels.to(torch.int64)\n\n        preds = network(images)\n        loss = loss_fn(preds, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        sched.step()\n\n        total_loss += loss.item() * len(batch[0])\n        total_correct += get_num_correct(preds, labels)\n\n    # Validation phase\n\n    total_val_loss = 0\n    total_val_correct = 0\n\n    network.eval()\n\n    for batch in val_loader:\n        images, labels = batch\n\n        images = images.to(device)\n        labels = labels.to(device)\n        labels = labels.to(torch.int64)\n\n        with torch.no_grad():\n            preds = network(images)\n            loss = loss_fn(preds, labels)\n\n        total_val_loss += loss.item() * len(batch[0])\n        total_val_correct += get_num_correct(preds, labels)\n\n    # Print out metrics\n\n    training_percentage = 100 * total_correct / train_dataset_size\n    validation_percentage = 100 * total_val_correct / val_dataset_size\n\n    print(f\"Epoch: {epoch + 1}\")\n\n    history_train_acc.append(training_percentage)\n    history_val_acc.append(validation_percentage)\n\n    history_train_loss.append(total_loss / train_dataset_size)\n    history_val_loss.append(total_val_loss / val_dataset_size)\n\n    print(f\"TRAINING     \"\n          f\"train accuracy {training_percentage:3.2f}%, \"\n          f\"train loss: {total_loss / train_dataset_size:2.4f}\")\n    print(f\"VALIDATION   \"\n          f\"val accuracy   {validation_percentage:3.2f}%, \"\n          f\"val loss:   {total_val_loss / val_dataset_size:2.4f}\")\n    print()\n    \nprint()\nprint()\npassed = time.time()-since\n\npassed_hrs = passed // 3600\npassed -= passed_hrs * 3600\n\npassed_mins = passed // 60\npassed -= passed_mins * 60\n\npassed_secs = int(passed)\n\nprint(f\"TIME of training (with validation phases) {passed_hrs}h {passed_mins}m {passed_secs}s.\")\n\"\"\"\nEND OF TRAINING\n\"\"\"","metadata":{"_uuid":"8fb935c8-97b3-465c-8312-1e009e79b317","_cell_guid":"b866be18-26b2-4afc-bb7f-3a737a12ed98","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T13:48:48.373302Z","iopub.execute_input":"2022-09-06T13:48:48.373661Z","iopub.status.idle":"2022-09-06T15:10:53.241941Z","shell.execute_reply.started":"2022-09-06T13:48:48.373625Z","shell.execute_reply":"2022-09-06T15:10:53.240879Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.plot(history_val_acc)\nplt.plot(history_train_acc)\nplt.ylabel('Accuracy (%)')\nplt.xlabel('No. epochs')\nplt.title('Accuracy through epochs')\nplt.legend([\"Validation\", \"Training\"])\nplt.show()","metadata":{"_uuid":"5d70b305-cc06-43ac-8350-b867dee56ce5","_cell_guid":"71b76018-dc7f-4b4a-b147-4f29cd7c0e70","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T15:10:53.243721Z","iopub.execute_input":"2022-09-06T15:10:53.244496Z","iopub.status.idle":"2022-09-06T15:10:53.487368Z","shell.execute_reply.started":"2022-09-06T15:10:53.244456Z","shell.execute_reply":"2022-09-06T15:10:53.486287Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nTESTING THE MODEL\n\"\"\"\n\npredictions = []\ntrue_labels = []\n\ntest_correct = 0\n\nnetwork.eval()\n\nfor batch in test_loader:\n    images, labels = batch\n    true_labels.extend(labels)\n\n    images = images.to(device)\n    labels = labels.to(device)\n    labels = labels.to(torch.int64)\n\n    with torch.no_grad():\n        preds = network(images)\n        predictions.extend((preds.argmax(dim=1)).tolist())\n        test_correct += get_num_correct(preds, labels)\n\nprint()\nprint(f\"Test set accuracy:\\t{100 * test_correct / test_dataset_size}%\")\nprint(test_correct)\nprint(test_dataset_size)","metadata":{"_uuid":"ed25fb9f-b95c-4222-9bd0-77cec778aa2c","_cell_guid":"18dde7dd-fde2-4626-ad7f-65414ac29f7a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T15:10:53.489796Z","iopub.execute_input":"2022-09-06T15:10:53.491632Z","iopub.status.idle":"2022-09-06T15:11:19.308161Z","shell.execute_reply.started":"2022-09-06T15:10:53.491594Z","shell.execute_reply":"2022-09-06T15:11:19.307114Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nCONFUSION MATRIX\n\"\"\"\n\ncm = confusion_matrix(true_labels, predictions)\n\nplt.figure(figsize=(12, 10))\nf = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nf.set_xticklabels(['Normal', 'Pneumonia'], rotation=40, size=16)\nf.set_yticklabels(['Normal', 'Pneumonia'], rotation=40, size=16)\nf.set_xlabel('True', size=24)\nf.set_ylabel('Predicted', size=24)\n\nf.set_title(\"Confusion Matrix\", size=32)","metadata":{"_uuid":"599e7f76-833b-4e99-9577-56f6d182270b","_cell_guid":"1fab4fc6-75cc-4b3c-9988-e741629e71c7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-06T15:11:19.309776Z","iopub.execute_input":"2022-09-06T15:11:19.310126Z","iopub.status.idle":"2022-09-06T15:11:19.720413Z","shell.execute_reply.started":"2022-09-06T15:11:19.310091Z","shell.execute_reply":"2022-09-06T15:11:19.719313Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"6f305bf9-43b4-4f5e-b771-ab253534ac08","_cell_guid":"a1c02edc-2d9b-4e7d-8bde-3c2fcfadd86a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}